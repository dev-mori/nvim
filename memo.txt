https://github.com/anthropics/claude-code
nodeのインストールとclaude-codeのインストール
`npm install -g @anthropic-ai/claude-code`

https://github.com/greggh/claude-code.nvim?tab=readme-ov-file

claude-codeがmodel変更できないっぽい

claude-codeでもエージェントっぽく行けるかもしれないが、コストが高いので安めでいくなら↓
cool-clineでsonnet3-haikuを使うのが良さそう
cool-clineはvscodeの拡張機能なのでvscodeで使う
https://github.com/yetone/avante.nvimだったらnvimでも使えそうだが、cursorベースっぽいので権限管理周りが冗長な可能性

haikuはだいぶ安いけど性能が少々心配
4oが定額で使えるとしたらcopilot agentのほうがいいかもしれない

copilot agentが定額でsonnet3.7とかも使い放題っぽい（一時的なレートリミットとかはありかもだけど）
これ最強かもしれない


フロントエンドテスト戦略
テスティングトロフィー

tanstack-query人気すごいな

負荷テストの504の要因は、ターゲットで処理できる量を超えたリクエストが飛んできて、スタックしてる可能性がある
504 Gateway Timeout → ターゲットがレスポンスを返さなかった

スムーズなスケールアウトのために
- CPU使用率などの閾値を低めてみる
- タスク起動の高速化（コンテナイメージの軽量化など）
- 最小タスクを２以上にしておく

localのlog_levelをinfoにしてタスクのログとおなじになるか確認してみる

cloudwatch logs insightsで、特定のロググループのログを検索する

fields @timestamp, @message
| filter @message like "ERROR" or @message like "Timeout" or @message like "Memory limit exceeded"
| sort @timestamp desc
| limit 100

こんなんとか

fields @timestamp, @message
| filter @logStream like "my-app"
| filter @message like "Request timed out"
| sort @timestamp desc
| limit 50

こんなんとか


メモリの使用量を確認する
# メモリを大量に消費しているプロセスを特定
ps aux --sort=-%mem | head -10
# メモリ使用量の推移をリアルタイムで監視
top -o %MEM
# プロセスのメモリキャッシュ状況を確認
free -h


distributed load testing (DLT)はJMeterで制御したほうがいいかもしれない(rpmとか)

あとcx２のリクエスト数見てたけど、ｃｘ２はヒートマップとかもろもろでもっと増えてるからstoreeelはもっと少なくなる



RPM（Requests Per Minute、1分あたりのリクエスト数）の計算例として、月間10万PVのサイトの場合で考えてみます。

均等分布の場合
1か月を30日、1日を24時間、1時間を60分とすると、1か月は 30 × 24 × 60 = 43,200 分。

均等に分布するなら、平均RPMは
  100,000 ÷ 43,200 ≒ 2.31 リクエスト／分

時間帯に偏りがある場合の例
たとえば、全PVの40%（40,000 PV）がピーク時に集中し、残り60%（60,000 PV）がオフピーク時に発生すると仮定します。

ピーク時：
 　ピーク時を1日あたり6時間（6 × 60 = 360分）とし、30日分で 360 × 30 = 10,800 分とすると、
  ピーク時のRPMは 40,000 ÷ 10,800 ≒ 3.70 リクエスト／分

オフピーク時：
 　残りの時間は 43,200 - 10,800 = 32,400 分なので、
  オフピーク時のRPMは 60,000 ÷ 32,400 ≒ 1.85 リクエスト／分

まとめ
平均RPM（全体で均等に分布した場合）：約 2.31 rpm

ピーク時のRPM（全PVの40%が6時間に集中）：約 3.70 rpm

オフピーク時のRPM：約 1.85 rpm



コスト計算は平均RPM 2.31 でテストして、負荷テストはピーク時のRPM 3.70 でテストするのが良さそう

一旦100万PVの場合は　1rpsくらいとしてコスト算出してみるか これでCPUやメモリ、RDSのACUがスケールアウトするギリギリくらいならそのままその費用で計算できる
 
VPCがやけにかかっていると思ったけど、多分動画を転送してるから思ってるよりかかってるっぽいな ほんとか？


rails version up
8.0.1 > 7.2.2.1 > 7.1.5.1 > 7.0.8.7


https://roadmap.sh/react
にしたがって技術取り入れていくか
特に最近UI周り気にされてるから
- Tailwind (CSS)
- Shadcn UI (UI)
- Radix UI (headless UI(behavior only))
- Framer Motion (animation)
あたりでコンポーネントをちゃんと作っていくのが良さそう

あと
- react-query
- react-hook-form
- react-router
とかは積極的に使っていくのが良さそう
余裕があれば storybook も入れたい







https://marketplace.visualstudio.com/items?itemName=figma.figma-vscode-extension
https://zenn.dev/aishift/articles/642583b257bedb


CPM以外のCPC、CPMは分析などが必要になるため、結構大変だし、広告を出す側と出される側どちらも大量に必要
純広告もしくはCPMならすんなり行けそう

出したいタグはいらなそう
目標CPAとかになる


CVを計測はGA4で行けそうに見える

```
utmパラメータを設定したときに、特定の流入経路から来た流入者がCVアクションを行った数をga4で見ることはできるか？


ChatGPT:
9 秒 の間、推論済み
はい、GA4ではUTMパラメータで流入経路を識別しているため、特定の流入経路（たとえば、utm_source、utm_medium、utm_campaign などで指定されたもの）から来たユーザーがどれだけCV（コンバージョン）アクションを行ったかを確認することが可能です。

具体的な実装方法
UTMパラメータの自動収集
GA4は、URLに含まれるutm_source、utm_medium、utm_campaignなどのパラメータを自動で取得し、キャンペーン情報として記録します。これにより、各流入チャネルに対する訪問者データが収集されます。

コンバージョンイベントの定義
広告プラットフォームなどで、ユーザーの購入や登録などの重要なアクションを「コンバージョンイベント」としてGA4上で設定します。この設定により、特定のイベントが「CV」として計上されるようになります。

エクスプロレーションレポート・セグメントの活用
GA4のエクスプロレーション機能やファネル分析、カスタムセグメントを作成し、特定のキャンペーンパラメータ（例: 特定のutm_sourceやutm_campaign）のユーザーのみを対象にしたレポートを作成します。これにより、その流入経路からの訪問者がコンバージョンに至った数を視覚的に確認できます。

まとめ
GA4はUTMパラメータで流入元を自動計測するため、キャンペーンごとにユーザーの行動とコンバージョンを追跡することが可能です。

コンバージョンイベントを正しく設定し、エクスプロレーションやセグメントを活用することで、特定の流入経路からのCV数を正確に把握できます。

こういった構成により、広告主やプラットフォーム側は、各キャンペーンの効果やROIの評価を行いやすくなります。
```

ただ、広告の入札システム（というか配信最適化？）がかなり大変そう

まずい　GA4の取得データ消せるっぽい

GA4は user_pseudo_id なるものを自動で生成して、クッキーやローカルストレージに保存して、イベントごとのセッション（ユーザー）の識別を行っているっぽい
